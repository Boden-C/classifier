{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "430ca6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c36393dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from src/extract/dataframe_checkpoint_20.pickle\n",
      "Loaded 19262 rows and 3 columns.\n",
      "Loaded 19262 rows and 3 columns.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "import os\n",
    "from src.utils import load_checkpoint\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "\n",
    "df = load_checkpoint(\"src/extract/dataframe_checkpoint_20.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa07716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features - ast tfidf vectorizer\n",
    "# model - logistic regression\n",
    "# Convert x_ast to string using .show()\n",
    "def ast_to_str(ast_obj):\n",
    "    buf = StringIO()\n",
    "    ast_obj.show(buf=buf)\n",
    "    return buf.getvalue()\n",
    "\n",
    "df['ast_str'] = df['x_ast'].apply(ast_to_str)\n",
    "\n",
    "X = df['ast_str']\n",
    "y = df['target']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61cc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing x_string with word analyzer\n",
      "Vectorizing x_string with char analyzer\n",
      "Vectorizing ast_str with word analyzer\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare input variants\n",
    "inputs = {\n",
    "    'ast_str': df['ast_str'],\n",
    "    'x_string': df['x_string']\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Vectorize features\n",
    "vectorizers = {\n",
    "    ('x_string', 'word'): TfidfVectorizer(analyzer='word', max_features=None, min_df=5, ngram_range=(3, 6)),\n",
    "    ('x_string', 'char'): TfidfVectorizer(analyzer='char', max_features=None, min_df=5, ngram_range=(3, 6)),\n",
    "    ('ast_str', 'word'): TfidfVectorizer(analyzer='word', max_features=None, min_df=5, ngram_range=(3, 6)),\n",
    "}\n",
    "\n",
    "X_matrices = {}\n",
    "for (input_name, analyzer), vectorizer in vectorizers.items():\n",
    "    print(f\"Vectorizing {input_name} with {analyzer} analyzer\")\n",
    "    X_matrices[(input_name, analyzer)] = vectorizer.fit_transform(df[input_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53b8488",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_matrices[('ast_str', 'word')] = vectorizer.fit_transform(df['ast_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f19aa0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression on x_string with word analyzer\n",
      "Input: x_string, Analyzer: word, Model: LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87      3098\n",
      "           1       0.48      0.63      0.54       755\n",
      "\n",
      "    accuracy                           0.79      3853\n",
      "   macro avg       0.69      0.73      0.71      3853\n",
      "weighted avg       0.82      0.79      0.80      3853\n",
      "\n",
      "Training LinearSVC on x_string with word analyzer\n",
      "Input: x_string, Analyzer: word, Model: LinearSVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.73      0.82      3098\n",
      "           1       0.43      0.81      0.56       755\n",
      "\n",
      "    accuracy                           0.75      3853\n",
      "   macro avg       0.68      0.77      0.69      3853\n",
      "weighted avg       0.84      0.75      0.77      3853\n",
      "\n",
      "Training LogisticRegression on x_string with char analyzer\n",
      "Input: x_string, Analyzer: char, Model: LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.87      0.91      3098\n",
      "           1       0.61      0.80      0.69       755\n",
      "\n",
      "    accuracy                           0.86      3853\n",
      "   macro avg       0.78      0.84      0.80      3853\n",
      "weighted avg       0.88      0.86      0.87      3853\n",
      "\n",
      "Training LinearSVC on x_string with char analyzer\n",
      "Input: x_string, Analyzer: char, Model: LinearSVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94      3098\n",
      "           1       0.75      0.81      0.78       755\n",
      "\n",
      "    accuracy                           0.91      3853\n",
      "   macro avg       0.85      0.87      0.86      3853\n",
      "weighted avg       0.91      0.91      0.91      3853\n",
      "\n",
      "Training LogisticRegression on ast_str with word analyzer\n",
      "Input: ast_str, Analyzer: word, Model: LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86      3098\n",
      "           1       0.46      0.61      0.52       755\n",
      "\n",
      "    accuracy                           0.78      3853\n",
      "   macro avg       0.68      0.72      0.69      3853\n",
      "weighted avg       0.81      0.78      0.79      3853\n",
      "\n",
      "Training LinearSVC on ast_str with word analyzer\n",
      "Input: ast_str, Analyzer: word, Model: LinearSVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90      3098\n",
      "           1       0.57      0.71      0.63       755\n",
      "\n",
      "    accuracy                           0.84      3853\n",
      "   macro avg       0.75      0.79      0.77      3853\n",
      "weighted avg       0.86      0.84      0.85      3853\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare model variants\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(solver='liblinear', max_iter=500, class_weight='balanced'),\n",
    "    'LinearSVC': LinearSVC(max_iter=500, class_weight='balanced'),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for (input_name, analyzer), X_mat in X_matrices.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_mat, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} on {input_name} with {analyzer} analyzer\")\n",
    "        if model_name == \"HistGradientBoostingClassifier\":\n",
    "            # Convert sparse to dense for HistGradientBoostingClassifier\n",
    "            X_train_fit = X_train.toarray()\n",
    "            X_test_fit = X_test.toarray()\n",
    "        else:\n",
    "            X_train_fit = X_train\n",
    "            X_test_fit = X_test\n",
    "        try:\n",
    "            model.fit(X_train_fit, y_train)\n",
    "            y_pred = model.predict(X_test_fit)\n",
    "        except Exception as e:\n",
    "            print(f\"Error for {input_name}, {analyzer}, {model_name}: {e}\")\n",
    "            continue\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        results[(input_name, analyzer, model_name)] = report\n",
    "        print(f\"Input: {input_name}, Analyzer: {analyzer}, Model: {model_name}\")\n",
    "        print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38a3278d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from src/extract/dataframe_checkpoint_20.pickle\n",
      "Loaded 19262 rows and 3 columns.\n",
      "Loaded 19262 rows and 3 columns.\n"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score, roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "df = load_checkpoint(\"src/extract/dataframe_checkpoint_20.pickle\")\n",
    "\n",
    "# load external sparse matrix\n",
    "with open(\"src/features/matrices/matrix_man-str.pickle\", \"rb\") as f:\n",
    "    man_str_matrix: csr_matrix = pickle.load(f)\n",
    "    \n",
    "def ast_to_str(ast_obj):\n",
    "    buf = StringIO()\n",
    "    ast_obj.show(buf=buf)\n",
    "    return buf.getvalue()\n",
    "\n",
    "df['ast_str'] = df['x_ast'].apply(ast_to_str)\n",
    "\n",
    "X = df['ast_str']\n",
    "y = df['target']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f386ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Combine TF-IDF of x_string with manual features by weight.\"\"\"\n",
    "    def __init__(self, analyzer: str = 'char', ngram_range: tuple = (4, 5), min_df: int = 1, weight: float = 0.1) -> None:\n",
    "        self.analyzer = analyzer\n",
    "        self.ngram_range = ngram_range\n",
    "        self.min_df = min_df\n",
    "        self.weight = weight\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        text = X['x_string']\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            analyzer=self.analyzer,\n",
    "            ngram_range=self.ngram_range,\n",
    "            min_df=self.min_df\n",
    "        )\n",
    "        self.vectorizer.fit(text)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        text = X['x_string']\n",
    "        tfidf_part = self.vectorizer.transform(text).multiply(1.0 - self.weight)\n",
    "        idx = X.index.to_numpy()\n",
    "        man_part = man_str_matrix[idx, :].multiply(self.weight)\n",
    "        return hstack([tfidf_part, man_part], format=\"csr\")\n",
    "\n",
    "neg = (y == 0).sum()\n",
    "pos = (y == 1).sum()\n",
    "scale_pos_weight = neg / pos\n",
    "\n",
    "initial_models = {\n",
    "    'svm': LinearSVC(class_weight='balanced'),\n",
    "    'logistic': LogisticRegression(solver='liblinear', class_weight='balanced'),\n",
    "    'decision_tree': DecisionTreeClassifier(class_weight='balanced'),\n",
    "    'random_forest': RandomForestClassifier(class_weight='balanced'),\n",
    "    'xgboost': XGBClassifier(use_label_encoder=False,\n",
    "                              eval_metric='logloss',\n",
    "                              scale_pos_weight=scale_pos_weight),\n",
    "}\n",
    "\n",
    "results_initial = {}\n",
    "X_df = df[['x_string']]\n",
    "\n",
    "# for name, clf in initial_models.items():\n",
    "#     pipe = Pipeline([('combined', CombinedFeatures()), ('clf', clf)])\n",
    "#     X_tr, X_ts, y_tr, y_ts = train_test_split(\n",
    "#         X_df, y, test_size=0.2, stratify=y, random_state=42\n",
    "#     )\n",
    "#     try:\n",
    "#         pipe.fit(X_tr, y_tr)\n",
    "#         preds = pipe.predict(X_ts)\n",
    "#     except Exception as err:\n",
    "#         print(f\"Error training {name}: {err}\")\n",
    "#         continue\n",
    "#     rpt = classification_report(y_ts, preds, output_dict=True)\n",
    "#     results_initial[name] = rpt\n",
    "#     print(f\"\\n{name} report:\\n\", classification_report(y_ts, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c94ae49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BayesSearchCV for random_forest\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning BayesSearchCV for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[43msearch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mx_string\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     \u001b[38;5;66;03m# Store detailed results in DataFrame\u001b[39;00m\n\u001b[32m     73\u001b[39m     cv_df = pd.DataFrame(search.cv_results_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\boden\\Documents\\Coding\\classifier\\.venv\\Lib\\site-packages\\skopt\\searchcv.py:542\u001b[39m, in \u001b[36mBayesSearchCV.fit\u001b[39m\u001b[34m(self, X, y, groups, callback, **fit_params)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.refit):\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    537\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mBayesSearchCV doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt support a callable refit, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    538\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt define an implicit score to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    539\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33moptimize\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    540\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[38;5;66;03m# BaseSearchCV never ranked train scores,\u001b[39;00m\n\u001b[32m    545\u001b[39m \u001b[38;5;66;03m# but apparently we used to ship this (back-compat)\u001b[39;00m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_train_score:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\boden\\Documents\\Coding\\classifier\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\boden\\Documents\\Coding\\classifier\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\boden\\Documents\\Coding\\classifier\\.venv\\Lib\\site-packages\\skopt\\searchcv.py:599\u001b[39m, in \u001b[36mBayesSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m n_iter > \u001b[32m0\u001b[39m:\n\u001b[32m    596\u001b[39m     \u001b[38;5;66;03m# when n_iter < n_points points left for evaluation\u001b[39;00m\n\u001b[32m    597\u001b[39m     n_points_adjusted = \u001b[38;5;28mmin\u001b[39m(n_iter, n_points)\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m     optim_result, score_name = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[43m        \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscore_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_points\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_points_adjusted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    606\u001b[39m     n_iter -= n_points\n\u001b[32m    608\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m eval_callbacks(callbacks, optim_result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\boden\\Documents\\Coding\\classifier\\.venv\\Lib\\site-packages\\skopt\\searchcv.py:453\u001b[39m, in \u001b[36mBayesSearchCV._step\u001b[39m\u001b[34m(self, search_space, optimizer, score_name, evaluate_candidates, n_points)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;66;03m# make lists into dictionaries\u001b[39;00m\n\u001b[32m    451\u001b[39m params_dict = [point_asdict(search_space, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params]\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m all_results = \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[38;5;66;03m# if self.scoring is a callable, we have to wait until here\u001b[39;00m\n\u001b[32m    456\u001b[39m \u001b[38;5;66;03m# to get the score name\u001b[39;00m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m score_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\boden\\Documents\\Coding\\classifier\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\boden\\Documents\\Coding\\classifier\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\boden\\Documents\\Coding\\classifier\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2071\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2065\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2071\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\boden\\Documents\\Coding\\classifier\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1681\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1678\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1680\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1681\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1683\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1684\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\boden\\Documents\\Coding\\classifier\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1799\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1789\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1794\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1797\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1798\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1799\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1800\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1802\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1803\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1810\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "search_spaces = {\n",
    "    'svm': {\n",
    "        'combined__min_df': Integer(1, 10),\n",
    "        'combined__weight': Real(0.0, 1.0, prior='uniform'),\n",
    "        'classifier__C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
    "        'classifier__loss': Categorical(['hinge', 'squared_hinge']),\n",
    "    },\n",
    "    'logistic': {\n",
    "        'combined__min_df': Integer(1, 10),\n",
    "        'combined__weight': Real(0.0, 1.0, prior='uniform'),\n",
    "        'classifier__C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'combined__min_df': Integer(1, 10),\n",
    "        'combined__weight': Real(0.0, 1.0, prior='uniform'),\n",
    "        'classifier__max_depth': Integer(1, 50),\n",
    "        'classifier__min_samples_split': Integer(2, 20),\n",
    "        'classifier__min_samples_leaf': Integer(1, 20),\n",
    "        'classifier__criterion': Categorical(['gini', 'entropy']),\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'combined__min_df': Integer(1, 10),\n",
    "        'combined__weight': Real(0.0, 1.0, prior='uniform'),\n",
    "        'classifier__max_depth': Integer(1, 50),\n",
    "        'classifier__min_samples_split': Integer(2, 20),\n",
    "        'classifier__min_samples_leaf': Integer(1, 20),\n",
    "        'classifier__max_features': Categorical(['sqrt', 'log2', None]),\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'combined__min_df': Integer(1, 10),\n",
    "        'combined__weight': Real(0.0, 1.0, prior='uniform'),\n",
    "        'classifier__n_estimators': Integer(50, 500),\n",
    "        'classifier__max_depth': Integer(1, 10),\n",
    "        'classifier__min_child_weight': Integer(1, 10),\n",
    "        'classifier__gamma': Real(0.0, 5.0),\n",
    "        'classifier__subsample': Real(0.5, 1.0),\n",
    "        'classifier__scale_pos_weight': Real(1.0, 10.0, prior='uniform'),\n",
    "    },\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'svm': LinearSVC(class_weight='balanced'),\n",
    "    'logistic': LogisticRegression(solver='liblinear', class_weight='balanced'),\n",
    "    'decision_tree': DecisionTreeClassifier(class_weight='balanced'),\n",
    "    'random_forest': RandomForestClassifier(class_weight='balanced'),\n",
    "    'xgboost': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "}\n",
    "\n",
    "results_skopt = {}\n",
    "i = 0\n",
    "skip = 3\n",
    "for name, estimator in models.items():\n",
    "    if i < skip:\n",
    "        i += 1\n",
    "        continue\n",
    "    pipe = Pipeline([\n",
    "        ('combined', CombinedFeatures()),\n",
    "        ('classifier', estimator),\n",
    "    ])\n",
    "    search = BayesSearchCV(\n",
    "        pipe,\n",
    "        search_spaces[name],\n",
    "        scoring=make_scorer(f1_score, pos_label=1),\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False,\n",
    "    )\n",
    "    print(f\"Running BayesSearchCV for {name}\")\n",
    "    try:\n",
    "        search.fit(df[['x_string']], y)\n",
    "        \n",
    "        # Store detailed results in DataFrame\n",
    "        cv_df = pd.DataFrame(search.cv_results_)\n",
    "        best_f1 = search.best_score_\n",
    "        best_params = search.best_params_\n",
    "        best_pipe = search.best_estimator_\n",
    "        \n",
    "        # Prepare result entry\n",
    "        entry = {\n",
    "            'cv_results': cv_df,\n",
    "            'best_f1_score': best_f1,\n",
    "            'best_params': best_params,\n",
    "            'best_estimator': best_pipe,\n",
    "        }\n",
    "        \n",
    "        # Compute ROC AUC via cross-validated predictions if possible\n",
    "        clf = best_pipe.named_steps['classifier']\n",
    "        if hasattr(clf, 'predict_proba'):\n",
    "            y_scores = cross_val_predict(best_pipe, df[['x_string']], y, cv=3, method='predict_proba')[:, 1]\n",
    "        elif hasattr(clf, 'decision_function'):\n",
    "            y_scores = cross_val_predict(best_pipe, df[['x_string']], y, cv=3, method='decision_function')\n",
    "        else:\n",
    "            y_scores = None\n",
    "            print(f\"Warning: Classifier for {name} has no predict_proba or decision_function; skipping ROC AUC.\")\n",
    "        \n",
    "        if y_scores is not None:\n",
    "            fpr, tpr, _ = roc_curve(y, y_scores, pos_label=1)\n",
    "            auc_score = auc(fpr, tpr)\n",
    "            entry.update({\n",
    "                'roc_auc_cv': auc_score,\n",
    "                'fpr_cv': fpr,\n",
    "                'tpr_cv': tpr,\n",
    "            })\n",
    "            print(f\"{name} best estimator ROC AUC (CV): {auc_score:.4f}\")\n",
    "        \n",
    "        results_skopt[name] = entry\n",
    "        # save to pickle checkpoint\n",
    "        with open(f\"skopt_results_{name}.pickle\", \"wb\") as f:\n",
    "            pickle.dump(results_skopt, f)\n",
    "        \n",
    "        \n",
    "    except Exception as err:\n",
    "        print(f\"Error in {name}: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81617d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9bab874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results to a pickle file\n",
    "with open(\"skopt_results.pickle\", \"wb\") as f:\n",
    "    pickle.dump(results_skopt, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
